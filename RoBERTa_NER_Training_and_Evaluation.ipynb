{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Posfay/Named-Entity-Recognition-using-BERT/blob/main/RoBERTa_NER_Training_and_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObqyrXEc-_Js"
      },
      "source": [
        "## Installs and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O20dNqx2-wYs",
        "outputId": "fa0ba173-1816-4e0e-edfb-14a822a095b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 451 kB 25.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 132 kB 76.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 74.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 81.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 79.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 33.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 33.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.9 MB/s \n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 72 kB 1.8 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install datasets -q\n",
        "!pip install tokenizers -q\n",
        "!pip install transformers -q\n",
        "!pip install seqeval -q\n",
        "!pip install evaluate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1C1AVdpj_GZ-"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from datasets import Dataset, Sequence, ClassLabel, Value\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from datasets import load_metric\n",
        "\n",
        "import evaluate\n",
        "import transformers\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForTokenClassification,\n",
        "    HfArgumentParser,\n",
        "    PushToHubCallback,\n",
        "    TFAutoModelForTokenClassification,\n",
        "    TFTrainingArguments,\n",
        "    create_optimizer,\n",
        "    set_seed,\n",
        "    pipeline,\n",
        ")\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXhtmBF-hi4a"
      },
      "source": [
        "## Splitting the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_Nm8BsQhi4b"
      },
      "outputs": [],
      "source": [
        "# Mount google drive first, then you can load the dataset\n",
        "not_tokenized_ds = load_from_disk(\"/content/drive/MyDrive/Colab Notebooks/sztaki_full_pretokenized_repaired\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Byx9vnOKhi4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49a090b9-7932-4fd1-e95b-dd6bf8c210d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached split indices for dataset at /content/drive/MyDrive/Colab Notebooks/sztaki_full_pretokenized_repaired/cache-6221d18299899c8b.arrow and /content/drive/MyDrive/Colab Notebooks/sztaki_full_pretokenized_repaired/cache-15045bd80a9066ae.arrow\n"
          ]
        }
      ],
      "source": [
        "# Creating a DatasetDict which contains a train and test dataset\n",
        "split_dataset = not_tokenized_ds.train_test_split(test_size=0.2, shuffle=True, seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYuqxCy5hi4c",
        "outputId": "02524906-b18c-42ba-c654-f964bd305543"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['tokens', 'ner_tags'],\n",
              "    num_rows: 1039987\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ],
      "source": [
        "split_dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibHEcJoThi4c"
      },
      "outputs": [],
      "source": [
        "# dictionaries for label to id conversion and vice versa (the model needs these)\n",
        "label_names = split_dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "id2label = {i: label for i, label in enumerate(label_names)}\n",
        "label2id = {v: k for k, v in id2label.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFDi-57shi4e"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTMhzo6Whi4e"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "sentence_max_length = 256\n",
        "max_train_samples = 5000\n",
        "max_eval_samples = 1000\n",
        "num_replicas = 1\n",
        "per_device_train_batch_size = 16\n",
        "per_device_eval_batch_size = 16\n",
        "total_train_batch_size = per_device_train_batch_size * num_replicas\n",
        "total_eval_batch_size = per_device_eval_batch_size * num_replicas\n",
        "num_train_epochs = 10\n",
        "learning_rate = 2e-5\n",
        "warmup_steps = 0\n",
        "warmup_ratio = 0\n",
        "return_entity_level_metrics = True\n",
        "output_dir = \"first_NER_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d35NbtTRhi4f"
      },
      "outputs": [],
      "source": [
        "# Loading a tokenizer - this is a multi language pretrained model\n",
        "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRSRsP8hhi4g"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(all_samples_per_split):\n",
        "    tokenized_samples = tokenizer(\n",
        "        all_samples_per_split[\"tokens\"],\n",
        "        max_length=sentence_max_length,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n",
        "        is_split_into_words=True,\n",
        "    )\n",
        "\n",
        "    # labels replacing ner_tags in the dataset\n",
        "    total_adjusted_labels = []\n",
        "\n",
        "    # correcting the labels (ner_tags) for every token because of subword tokenization\n",
        "    for k in range(0, len(tokenized_samples[\"input_ids\"])):\n",
        "        prev_wid = -1\n",
        "        word_ids_list = tokenized_samples.word_ids(batch_index=k)\n",
        "        existing_label_ids = all_samples_per_split[\"ner_tags\"][k]\n",
        "        i = -1\n",
        "        # labels replacing ner_tags in the current sequence\n",
        "        adjusted_label_ids = []\n",
        "\n",
        "        for word_idx in word_ids_list:\n",
        "            # Subword tokens have a word id that is None. We set the label to -100 \n",
        "            # so they are automatically ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                adjusted_label_ids.append(-100)\n",
        "            # if next token is a subword token, mark with same label (ner_tag)\n",
        "            elif word_idx != prev_wid:\n",
        "                i = i + 1\n",
        "                adjusted_label_ids.append(existing_label_ids[i])\n",
        "                prev_wid = word_idx\n",
        "            # if next token is a new word, add the correct label to the list\n",
        "            else:\n",
        "                # label_name = label_names[existing_label_ids[i]]\n",
        "                adjusted_label_ids.append(existing_label_ids[i])\n",
        "\n",
        "        # add current sequence's corrected labels to the dataset\n",
        "        total_adjusted_labels.append(adjusted_label_ids)\n",
        "\n",
        "    # add adjusted labels to the tokenized dataset\n",
        "    tokenized_samples[\"labels\"] = total_adjusted_labels\n",
        "    return tokenized_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhzXDaBEhi4h",
        "outputId": "d85d0132-efdd-49fd-ff8a-744833eeb6e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /content/drive/MyDrive/Colab Notebooks/sztaki_full_pretokenized_repaired/cache-beb06714a3548c95.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /content/drive/MyDrive/Colab Notebooks/sztaki_full_pretokenized_repaired/cache-7c1adc31316214ff.arrow\n"
          ]
        }
      ],
      "source": [
        "# Tokenization on the datasets\n",
        "processed_raw_datasets = split_dataset.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=True,\n",
        "    remove_columns=split_dataset[\"train\"].column_names,\n",
        "    desc=\"Running tokenizer on dataset\",\n",
        ")\n",
        "\n",
        "train_dataset = processed_raw_datasets[\"train\"]\n",
        "eval_dataset = processed_raw_datasets[\"test\"]\n",
        "\n",
        "# Limiting the number of train or eval samples if specified\n",
        "if max_train_samples > 0:\n",
        "    max_train_samples = min(len(train_dataset), max_train_samples)\n",
        "    train_dataset = train_dataset.select(range(max_train_samples))\n",
        "\n",
        "if max_eval_samples > 0:\n",
        "    max_eval_samples = min(len(eval_dataset), max_eval_samples)\n",
        "    eval_dataset = eval_dataset.select(range(max_eval_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "599cDld_hi4h",
        "outputId": "9c80d104-0e53-4948-a228-15e3192339e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFXLMRobertaForTokenClassification.\n",
            "\n",
            "Some layers of TFXLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Loading a pretrained model for token classification - this is a multi language pretrained model\n",
        "model = TFAutoModelForTokenClassification.from_pretrained(\n",
        "    \"xlm-roberta-base\",\n",
        "    num_labels=len(label_names),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2H4kZS3hi4h"
      },
      "outputs": [],
      "source": [
        "# We resize the embeddings only when necessary to avoid index errors\n",
        "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
        "if len(tokenizer) > embedding_size:\n",
        "    model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7TCcy_yhi4i"
      },
      "outputs": [],
      "source": [
        "# We need the DataCollatorForTokenClassification here, \n",
        "# as we need to correctly pad labels as well as inputs.\n",
        "collate_fn = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAYg65Pmhi4i"
      },
      "outputs": [],
      "source": [
        "dataset_options = tf.data.Options()\n",
        "dataset_options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bx7GNXXhi4i",
        "outputId": "f9c73c04-7b74-41df-ca8f-227366f7b727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        }
      ],
      "source": [
        "# Converting the HuggingFace datasets to Tensorflow.data.Dataset\n",
        "tf_train_dataset = model.prepare_tf_dataset(\n",
        "    train_dataset,\n",
        "    collate_fn=collate_fn,\n",
        "    batch_size=total_train_batch_size,\n",
        "    shuffle=True,\n",
        ").with_options(dataset_options)\n",
        "\n",
        "tf_eval_dataset = model.prepare_tf_dataset(\n",
        "    eval_dataset,\n",
        "    collate_fn=collate_fn,\n",
        "    batch_size=total_eval_batch_size,\n",
        "    shuffle=False,\n",
        ").with_options(dataset_options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drQ3PEfAhi4j",
        "outputId": "3fb36507-2224-4d4b-a8a6-e346900068ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        }
      ],
      "source": [
        "num_train_steps = int(len(tf_train_dataset) * num_train_epochs)\n",
        "if warmup_steps > 0:\n",
        "    num_warmup_steps = warmup_steps\n",
        "elif warmup_ratio > 0:\n",
        "    num_warmup_steps = int(num_train_steps * warmup_ratio)\n",
        "else:\n",
        "    num_warmup_steps = 0\n",
        "\n",
        "# Creating an optimizer for the model\n",
        "optimizer, lr_schedule = create_optimizer(\n",
        "    init_lr=learning_rate,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    # adam_beta1=adam_beta1,\n",
        "    # adam_beta2=adam_beta2,\n",
        "    # adam_epsilon=adam_epsilon,\n",
        "    # weight_decay_rate=weight_decay,\n",
        "    # adam_global_clipnorm=max_grad_norm,\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5WriON0hi4j"
      },
      "outputs": [],
      "source": [
        "# Creating the evaluation function for the model\n",
        "\n",
        "metric = evaluate.load(\"seqeval\")\n",
        "\n",
        "def get_labels(y_pred, y_true):\n",
        "    # Transform predictions and references tensors to numpy arrays\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_names[p] for (p, l) in zip(pred, gold_label) if l != -100]\n",
        "        for pred, gold_label in zip(y_pred, y_true)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_names[l] for (p, l) in zip(pred, gold_label) if l != -100]\n",
        "        for pred, gold_label in zip(y_pred, y_true)\n",
        "    ]\n",
        "    return true_predictions, true_labels\n",
        "\n",
        "def compute_metrics():\n",
        "    results = metric.compute()\n",
        "    if return_entity_level_metrics:\n",
        "        # Unpack nested dictionaries\n",
        "        final_results = {}\n",
        "        for key, value in results.items():\n",
        "            if isinstance(value, dict):\n",
        "                for n, v in value.items():\n",
        "                    final_results[f\"{key}_{n}\"] = v\n",
        "            else:\n",
        "                final_results[key] = value\n",
        "        return final_results\n",
        "    else:\n",
        "        return {\n",
        "            \"precision\": results[\"overall_precision\"],\n",
        "            \"recall\": results[\"overall_recall\"],\n",
        "            \"f1\": results[\"overall_f1\"],\n",
        "            \"accuracy\": results[\"overall_accuracy\"],\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mvbVem3Ohi4j",
        "outputId": "375330ec-c511-4055-fcba-4f63638f1f74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:***** Running training *****\n",
            "INFO:root:  Num examples = 5000\n",
            "INFO:root:  Num Epochs = 10\n",
            "INFO:root:  Instantaneous batch size per device = 16\n",
            "INFO:root:  Total train batch size = 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "  5/312 [..............................] - ETA: 4:33 - loss: 2.0710"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-140-e6cec1af79d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Total train batch size = {total_train_batch_size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtf_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_eval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'tfxlm_roberta_for_token_classification_3/roberta/encoder/layer_._11/output/LayerNorm/batchnorm/mul_2' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 985, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 612, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 149, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py\", line 690, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py\", line 743, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 787, in inner\n      self.run()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 748, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 543, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2854, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3057, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-140-e6cec1af79d8>\", line 7, in <module>\n      model.fit(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_tf_utils.py\", line 1520, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_tf_utils.py\", line 1214, in run_call_with_unpacked_inputs\n      raise NotImplementedError\n    File \"/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_tf_roberta.py\", line 1584, in call\n      outputs = self.roberta(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_tf_utils.py\", line 1214, in run_call_with_unpacked_inputs\n      raise NotImplementedError\n    File \"/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_tf_roberta.py\", line 747, in call\n      encoder_outputs = self.encoder(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_tf_roberta.py\", line 538, in call\n      for i, layer_module in enumerate(self.layer):\n    File \"/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_tf_roberta.py\", line 544, in call\n      layer_outputs = layer_module(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_tf_roberta.py\", line 500, in call\n      layer_output = self.bert_output(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_tf_roberta.py\", line 420, in call\n      hidden_states = self.LayerNorm(inputs=hidden_states + input_tensor)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/layers/normalization/layer_normalization.py\", line 275, in call\n      outputs = tf.nn.batch_normalization(\nNode: 'tfxlm_roberta_for_token_classification_3/roberta/encoder/layer_._11/output/LayerNorm/batchnorm/mul_2'\nfailed to allocate memory\n\t [[{{node tfxlm_roberta_for_token_classification_3/roberta/encoder/layer_._11/output/LayerNorm/batchnorm/mul_2}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_200922]"
          ]
        }
      ],
      "source": [
        "logger.info(\"***** Running training *****\")\n",
        "logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
        "logger.info(f\"  Num Epochs = {num_train_epochs}\")\n",
        "logger.info(f\"  Instantaneous batch size per device = {per_device_train_batch_size}\")\n",
        "logger.info(f\"  Total train batch size = {total_train_batch_size}\")\n",
        "\n",
        "model.fit(\n",
        "    tf_train_dataset,\n",
        "    validation_data=tf_eval_dataset,\n",
        "    epochs=int(num_train_epochs)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqezD6Fuhi4k",
        "outputId": "c578ffbd-723e-4f5c-8c9c-e0377a7387c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 21s 281ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Evaluation metrics:\n",
            "INFO:root:LOC_precision: 0.0000\n",
            "INFO:root:LOC_recall: 0.0000\n",
            "INFO:root:LOC_f1: 0.0000\n",
            "INFO:root:LOC_number: 378.0000\n",
            "INFO:root:MISC_precision: 0.0000\n",
            "INFO:root:MISC_recall: 0.0000\n",
            "INFO:root:MISC_f1: 0.0000\n",
            "INFO:root:MISC_number: 20.0000\n",
            "INFO:root:ORG_precision: 0.0005\n",
            "INFO:root:ORG_recall: 0.1500\n",
            "INFO:root:ORG_f1: 0.0010\n",
            "INFO:root:ORG_number: 20.0000\n",
            "INFO:root:PER_precision: 0.0000\n",
            "INFO:root:PER_recall: 0.0000\n",
            "INFO:root:PER_f1: 0.0000\n",
            "INFO:root:PER_number: 233.0000\n",
            "INFO:root:overall_precision: 0.0005\n",
            "INFO:root:overall_recall: 0.0046\n",
            "INFO:root:overall_f1: 0.0009\n",
            "INFO:root:overall_accuracy: 0.4446\n"
          ]
        }
      ],
      "source": [
        "# Getting the predictions for the validation dataset\n",
        "predictions = model.predict(tf_eval_dataset, batch_size=per_device_eval_batch_size)[\"logits\"]\n",
        "# Leaving only the most likely label for each token\n",
        "predictions = tf.math.argmax(predictions, axis=-1).numpy()\n",
        "labels = eval_dataset.with_format(\"tf\")[\"labels\"]\n",
        "labels = labels.numpy()\n",
        "# Hiding the predictions for any token that is hidden on the input\n",
        "attention_mask = eval_dataset.with_format(\"tf\")[\"attention_mask\"]\n",
        "attention_mask = attention_mask.numpy()\n",
        "labels[attention_mask == 0] = -100\n",
        "# Retrieving the true predictions and labels (excluding hidden tokens)\n",
        "preds, refs = get_labels(predictions, labels)\n",
        "metric.add_batch(\n",
        "    predictions=preds,\n",
        "    references=refs,\n",
        ")\n",
        "# Calculating and printing the metrics\n",
        "eval_metric = compute_metrics()\n",
        "logger.info(\"Evaluation metrics:\")\n",
        "for key, val in eval_metric.items():\n",
        "    logger.info(f\"{key}: {val:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGPTFzt7hi4k"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fAL_rgp0NOr"
      },
      "source": [
        "## Predicting with the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFoX0xrH7uDE"
      },
      "outputs": [],
      "source": [
        "# Using HuggingFace pipeline we can create a ready-to-use model from the one \n",
        "# fine-tuned before\n",
        "token_classifier = pipeline(\n",
        "    \"token-classification\",\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    aggregation_strategy=\"simple\", \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHBhVKzJ0S8c"
      },
      "outputs": [],
      "source": [
        "input_text = \"Pista megette az összes hamburgert Los Angelesben miután találkozott Trumppal\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SatPyOiL8eN6",
        "outputId": "04fe35c8-8627-46c4-924c-c1c5ac022a90"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER',\n",
              "  'score': 0.96302074,\n",
              "  'word': 'P',\n",
              "  'start': 0,\n",
              "  'end': 1},\n",
              " {'entity_group': 'PER',\n",
              "  'score': 0.95925903,\n",
              "  'word': 'ista',\n",
              "  'start': 1,\n",
              "  'end': 5},\n",
              " {'entity_group': 'LOC',\n",
              "  'score': 0.9326019,\n",
              "  'word': 'Los Angelesben',\n",
              "  'start': 35,\n",
              "  'end': 49},\n",
              " {'entity_group': 'PER',\n",
              "  'score': 0.98374766,\n",
              "  'word': 'István',\n",
              "  'start': 69,\n",
              "  'end': 75},\n",
              " {'entity_group': 'PER',\n",
              "  'score': 0.9753255,\n",
              "  'word': 'nal',\n",
              "  'start': 75,\n",
              "  'end': 78}]"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "token_classifier(input_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}