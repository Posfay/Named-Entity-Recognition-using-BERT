{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Posfay/Named-Entity-Recognition-using-BERT/blob/main/NER_Training_and_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObqyrXEc-_Js"
      },
      "source": [
        "## Installs and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O20dNqx2-wYs"
      },
      "outputs": [],
      "source": [
        "!pip install datasets -q\n",
        "!pip install tokenizers -q\n",
        "!pip install transformers -q\n",
        "!pip install seqeval -q\n",
        "!pip install evaluate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1C1AVdpj_GZ-"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from datasets import Dataset, Sequence, ClassLabel, Value\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from datasets import load_metric\n",
        "\n",
        "import evaluate\n",
        "import transformers\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForTokenClassification,\n",
        "    HfArgumentParser,\n",
        "    PushToHubCallback,\n",
        "    TFAutoModelForTokenClassification,\n",
        "    TFTrainingArguments,\n",
        "    create_optimizer,\n",
        "    set_seed,\n",
        "    pipeline,\n",
        ")\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "CXhtmBF-hi4a"
      },
      "source": [
        "## Splitting the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_Nm8BsQhi4b"
      },
      "outputs": [],
      "source": [
        "# Mount google drive first, then you can load the dataset\n",
        "not_tokenized_ds = load_from_disk(\"/content/drive/MyDrive/sztaki_full_pretokenized_repaired\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Byx9vnOKhi4b",
        "outputId": "019356e5-8227-43b1-9cb7-79dcc252d583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached split indices for dataset at /content/drive/MyDrive/sztaki_full_pretokenized_repaired/cache-bc368acdcdd8cabd.arrow and /content/drive/MyDrive/sztaki_full_pretokenized_repaired/cache-1153b66822735461.arrow\n"
          ]
        }
      ],
      "source": [
        "# Creating a DatasetDict which contains a train and test dataset\n",
        "split_dataset = not_tokenized_ds.train_test_split(test_size=0.2, shuffle=True, seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYuqxCy5hi4c",
        "outputId": "e6fe66fe-832e-4a28-a4e1-a3fc69d21337"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['tokens', 'ner_tags'],\n",
              "    num_rows: 1039987\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "split_dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibHEcJoThi4c"
      },
      "outputs": [],
      "source": [
        "# dictionaries for label to id conversion and vice versa (the model needs these)\n",
        "label_names = split_dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "id2label = {i: label for i, label in enumerate(label_names)}\n",
        "label2id = {v: k for k, v in id2label.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFDi-57shi4e"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTMhzo6Whi4e"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "sentence_max_length = 256\n",
        "max_train_samples = 5000\n",
        "max_eval_samples = 1000\n",
        "num_replicas = 1\n",
        "per_device_train_batch_size = 16\n",
        "per_device_eval_batch_size = 16\n",
        "total_train_batch_size = per_device_train_batch_size * num_replicas\n",
        "total_eval_batch_size = per_device_eval_batch_size * num_replicas\n",
        "num_train_epochs = 2\n",
        "learning_rate = 2e-5\n",
        "warmup_steps = 0\n",
        "warmup_ratio = 0\n",
        "return_entity_level_metrics = True\n",
        "output_dir = \"first_NER_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d35NbtTRhi4f"
      },
      "outputs": [],
      "source": [
        "# Loading a tokenizer - this is a hungarian pretrained model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"SZTAKI-HLT/hubert-base-cc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRSRsP8hhi4g"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(all_samples_per_split):\n",
        "    tokenized_samples = tokenizer(\n",
        "        all_samples_per_split[\"tokens\"],\n",
        "        max_length=sentence_max_length,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n",
        "        is_split_into_words=True,\n",
        "    )\n",
        "\n",
        "    # labels replacing ner_tags in the dataset\n",
        "    total_adjusted_labels = []\n",
        "\n",
        "    # correcting the labels (ner_tags) for every token because of subword tokenization\n",
        "    for k in range(0, len(tokenized_samples[\"input_ids\"])):\n",
        "        prev_wid = -1\n",
        "        word_ids_list = tokenized_samples.word_ids(batch_index=k)\n",
        "        existing_label_ids = all_samples_per_split[\"ner_tags\"][k]\n",
        "        i = -1\n",
        "        # labels replacing ner_tags in the current sequence\n",
        "        adjusted_label_ids = []\n",
        "\n",
        "        for word_idx in word_ids_list:\n",
        "            # Subword tokens have a word id that is None. We set the label to -100 \n",
        "            # so they are automatically ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                adjusted_label_ids.append(-100)\n",
        "            # if next token is a subword token, mark with same label (ner_tag)\n",
        "            elif word_idx != prev_wid:\n",
        "                i = i + 1\n",
        "                adjusted_label_ids.append(existing_label_ids[i])\n",
        "                prev_wid = word_idx\n",
        "            # if next token is a new word, add the correct label to the list\n",
        "            else:\n",
        "                # label_name = label_names[existing_label_ids[i]]\n",
        "                adjusted_label_ids.append(existing_label_ids[i])\n",
        "\n",
        "        # add current sequence's corrected labels to the dataset\n",
        "        total_adjusted_labels.append(adjusted_label_ids)\n",
        "\n",
        "    # add adjusted labels to the tokenized dataset\n",
        "    tokenized_samples[\"labels\"] = total_adjusted_labels\n",
        "    return tokenized_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhzXDaBEhi4h",
        "outputId": "9913900e-3fdb-43b3-b4f6-600ce4e52a6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /content/drive/MyDrive/sztaki_full_pretokenized_repaired/cache-35064353f2c466c7.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /content/drive/MyDrive/sztaki_full_pretokenized_repaired/cache-ce8d0ca40ba5617b.arrow\n"
          ]
        }
      ],
      "source": [
        "# Tokenization on the datasets\n",
        "processed_raw_datasets = split_dataset.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=True,\n",
        "    remove_columns=split_dataset[\"train\"].column_names,\n",
        "    desc=\"Running tokenizer on dataset\",\n",
        ")\n",
        "\n",
        "train_dataset = processed_raw_datasets[\"train\"]\n",
        "eval_dataset = processed_raw_datasets[\"test\"]\n",
        "\n",
        "# Limiting the number of train or eval samples if specified\n",
        "if max_train_samples > 0:\n",
        "    max_train_samples = min(len(train_dataset), max_train_samples)\n",
        "    train_dataset = train_dataset.select(range(max_train_samples))\n",
        "\n",
        "if max_eval_samples > 0:\n",
        "    max_eval_samples = min(len(eval_dataset), max_eval_samples)\n",
        "    eval_dataset = eval_dataset.select(range(max_eval_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "599cDld_hi4h",
        "outputId": "47e90c7b-d2fe-4617-f048-8871645f0fb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at SZTAKI-HLT/hubert-base-cc and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Loading a pretrained model for token classification - this is a hungarian pretrained model\n",
        "model = TFAutoModelForTokenClassification.from_pretrained(\n",
        "    \"SZTAKI-HLT/hubert-base-cc\",\n",
        "    num_labels=len(label_names),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2H4kZS3hi4h"
      },
      "outputs": [],
      "source": [
        "# We resize the embeddings only when necessary to avoid index errors\n",
        "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
        "if len(tokenizer) > embedding_size:\n",
        "    model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7TCcy_yhi4i"
      },
      "outputs": [],
      "source": [
        "# We need the DataCollatorForTokenClassification here, \n",
        "# as we need to correctly pad labels as well as inputs.\n",
        "collate_fn = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAYg65Pmhi4i"
      },
      "outputs": [],
      "source": [
        "dataset_options = tf.data.Options()\n",
        "dataset_options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bx7GNXXhi4i",
        "outputId": "f183f6fc-1cbd-4ac3-f12d-6fce03882fce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        }
      ],
      "source": [
        "# Converting the HuggingFace datasets to Tensorflow.data.Dataset\n",
        "tf_train_dataset = model.prepare_tf_dataset(\n",
        "    train_dataset,\n",
        "    collate_fn=collate_fn,\n",
        "    batch_size=total_train_batch_size,\n",
        "    shuffle=True,\n",
        ").with_options(dataset_options)\n",
        "\n",
        "tf_eval_dataset = model.prepare_tf_dataset(\n",
        "    eval_dataset,\n",
        "    collate_fn=collate_fn,\n",
        "    batch_size=total_eval_batch_size,\n",
        "    shuffle=False,\n",
        ").with_options(dataset_options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drQ3PEfAhi4j",
        "outputId": "a1c266ac-ebc7-4c28-d6c2-a039f858725b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        }
      ],
      "source": [
        "num_train_steps = int(len(tf_train_dataset) * num_train_epochs)\n",
        "if warmup_steps > 0:\n",
        "    num_warmup_steps = warmup_steps\n",
        "elif warmup_ratio > 0:\n",
        "    num_warmup_steps = int(num_train_steps * warmup_ratio)\n",
        "else:\n",
        "    num_warmup_steps = 0\n",
        "\n",
        "# Creating an optimizer for the model\n",
        "optimizer, lr_schedule = create_optimizer(\n",
        "    init_lr=learning_rate,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    # adam_beta1=adam_beta1,\n",
        "    # adam_beta2=adam_beta2,\n",
        "    # adam_epsilon=adam_epsilon,\n",
        "    # weight_decay_rate=weight_decay,\n",
        "    # adam_global_clipnorm=max_grad_norm,\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5WriON0hi4j"
      },
      "outputs": [],
      "source": [
        "# Creating the evaluation function for the model\n",
        "\n",
        "metric = evaluate.load(\"seqeval\")\n",
        "\n",
        "def get_labels(y_pred, y_true):\n",
        "    # Transform predictions and references tensors to numpy arrays\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_names[p] for (p, l) in zip(pred, gold_label) if l != -100]\n",
        "        for pred, gold_label in zip(y_pred, y_true)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_names[l] for (p, l) in zip(pred, gold_label) if l != -100]\n",
        "        for pred, gold_label in zip(y_pred, y_true)\n",
        "    ]\n",
        "    return true_predictions, true_labels\n",
        "\n",
        "def compute_metrics():\n",
        "    results = metric.compute()\n",
        "    if return_entity_level_metrics:\n",
        "        # Unpack nested dictionaries\n",
        "        final_results = {}\n",
        "        for key, value in results.items():\n",
        "            if isinstance(value, dict):\n",
        "                for n, v in value.items():\n",
        "                    final_results[f\"{key}_{n}\"] = v\n",
        "            else:\n",
        "                final_results[key] = value\n",
        "        return final_results\n",
        "    else:\n",
        "        return {\n",
        "            \"precision\": results[\"overall_precision\"],\n",
        "            \"recall\": results[\"overall_recall\"],\n",
        "            \"f1\": results[\"overall_f1\"],\n",
        "            \"accuracy\": results[\"overall_accuracy\"],\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "mvbVem3Ohi4j",
        "outputId": "9d6a7480-235a-45ba-eade-b5f56bce76ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:***** Running training *****\n",
            "INFO:root:  Num examples = 5000\n",
            "INFO:root:  Num Epochs = 2\n",
            "INFO:root:  Instantaneous batch size per device = 16\n",
            "INFO:root:  Total train batch size = 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "  1/312 [..............................] - ETA: 4:39:29 - loss: 1.1885"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-e6cec1af79d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtf_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_eval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2454\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "logger.info(\"***** Running training *****\")\n",
        "logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
        "logger.info(f\"  Num Epochs = {num_train_epochs}\")\n",
        "logger.info(f\"  Instantaneous batch size per device = {per_device_train_batch_size}\")\n",
        "logger.info(f\"  Total train batch size = {total_train_batch_size}\")\n",
        "\n",
        "model.fit(\n",
        "    tf_train_dataset,\n",
        "    validation_data=tf_eval_dataset,\n",
        "    epochs=int(num_train_epochs)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqezD6Fuhi4k",
        "outputId": "76cc6ac5-249a-4084-9f64-0b22e8466079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 20s 530ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Evaluation metrics:\n",
            "INFO:root:LOC_precision: 0.9272\n",
            "INFO:root:LOC_recall: 0.9910\n",
            "INFO:root:LOC_f1: 0.9580\n",
            "INFO:root:LOC_number: 334.0000\n",
            "INFO:root:MISC_precision: 0.8947\n",
            "INFO:root:MISC_recall: 0.7727\n",
            "INFO:root:MISC_f1: 0.8293\n",
            "INFO:root:MISC_number: 22.0000\n",
            "INFO:root:ORG_precision: 0.5652\n",
            "INFO:root:ORG_recall: 0.7647\n",
            "INFO:root:ORG_f1: 0.6500\n",
            "INFO:root:ORG_number: 17.0000\n",
            "INFO:root:PER_precision: 0.8908\n",
            "INFO:root:PER_recall: 0.8465\n",
            "INFO:root:PER_f1: 0.8681\n",
            "INFO:root:PER_number: 241.0000\n",
            "INFO:root:overall_precision: 0.8997\n",
            "INFO:root:overall_recall: 0.9202\n",
            "INFO:root:overall_f1: 0.9098\n",
            "INFO:root:overall_accuracy: 0.9961\n"
          ]
        }
      ],
      "source": [
        "# Getting the predictions for the validation dataset\n",
        "predictions = model.predict(tf_eval_dataset, batch_size=per_device_eval_batch_size)[\"logits\"]\n",
        "# Leaving only the most likely label for each token\n",
        "predictions = tf.math.argmax(predictions, axis=-1).numpy()\n",
        "labels = eval_dataset.with_format(\"tf\")[\"labels\"]\n",
        "labels = labels.numpy()\n",
        "# Hiding the predictions for any token that is hidden on the input\n",
        "attention_mask = eval_dataset.with_format(\"tf\")[\"attention_mask\"]\n",
        "attention_mask = attention_mask.numpy()\n",
        "labels[attention_mask == 0] = -100\n",
        "# Retrieving the true predictions and labels (excluding hidden tokens)\n",
        "preds, refs = get_labels(predictions, labels)\n",
        "metric.add_batch(\n",
        "    predictions=preds,\n",
        "    references=refs,\n",
        ")\n",
        "# Calculating and printing the metrics\n",
        "eval_metric = compute_metrics()\n",
        "logger.info(\"Evaluation metrics:\")\n",
        "for key, val in eval_metric.items():\n",
        "    logger.info(f\"{key}: {val:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGPTFzt7hi4k"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicting with the model"
      ],
      "metadata": {
        "id": "2fAL_rgp0NOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using HuggingFace pipeline we can create a ready-to-use model from the one \n",
        "# fine-tuned before\n",
        "token_classifier = pipeline(\n",
        "    \"token-classification\",\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    aggregation_strategy=\"simple\", \n",
        ")"
      ],
      "metadata": {
        "id": "rFoX0xrH7uDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Pista megette az összes hamburgert Los Angelesben miután találkozott Trumppal\""
      ],
      "metadata": {
        "id": "sHBhVKzJ0S8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_classifier(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SatPyOiL8eN6",
        "outputId": "586af932-07be-4023-b81e-3dd3ed3de101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity_group': 'LABEL_1',\n",
              "  'score': 0.91885906,\n",
              "  'word': 'Pista',\n",
              "  'start': 0,\n",
              "  'end': 5},\n",
              " {'entity_group': 'LABEL_0',\n",
              "  'score': 0.9995634,\n",
              "  'word': 'megette az összes hamburgert',\n",
              "  'start': 6,\n",
              "  'end': 34},\n",
              " {'entity_group': 'LABEL_3',\n",
              "  'score': 0.961508,\n",
              "  'word': 'Los Angelesben',\n",
              "  'start': 35,\n",
              "  'end': 49},\n",
              " {'entity_group': 'LABEL_0',\n",
              "  'score': 0.9994347,\n",
              "  'word': 'miután találkozott',\n",
              "  'start': 50,\n",
              "  'end': 68},\n",
              " {'entity_group': 'LABEL_1',\n",
              "  'score': 0.9570334,\n",
              "  'word': 'Trumppal',\n",
              "  'start': 69,\n",
              "  'end': 77}]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}